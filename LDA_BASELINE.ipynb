{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LDA_BASELINE.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAYC9XQWzBAr",
        "outputId": "cf88a7c1-f808-4d4c-93db-6915d67d0893"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "import spacy\n",
        "# Gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "from pprint import pprint\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0C_dhuFzSbU"
      },
      "source": [
        "medium_articles = pd.read_csv(\"articles.csv\")\n",
        "medium_articles.text = medium_articles.text.apply(lambda t : t.lower())\n",
        "# Dictionary of English Contractions\n",
        "contractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n",
        "                     \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n",
        "                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n",
        "                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n",
        "                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
        "                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n",
        "                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n",
        "                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n",
        "                     \"I'd\": \"I would\", \"I'd've\": \"I would have\",\"I'll\": \"I will\",\n",
        "                     \"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\", \"isn't\": \"is not\",\n",
        "                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n",
        "                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n",
        "                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n",
        "                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n",
        "                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
        "                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n",
        "                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
        "                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n",
        "                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n",
        "                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n",
        "                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n",
        "                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n",
        "                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n",
        "                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n",
        "                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n",
        "                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n",
        "                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n",
        "                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n",
        "                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n",
        "                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n",
        "                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n",
        "                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n",
        "                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
        "                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
        "                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n",
        "                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n",
        "                     \"you've\": \"you have\"}\n",
        "\n",
        "# Regular expression for finding contractions\n",
        "contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
        "\n",
        "# Function for expanding contractions\n",
        "def expand_contractions(text,contractions_dict=contractions_dict):\n",
        "    def replace(match):\n",
        "        return contractions_dict[match.group(0)]\n",
        "    return contractions_re.sub(replace, text)\n",
        "\n",
        "# Expanding Contractions in the text data\n",
        "medium_articles.text = medium_articles.text.apply(lambda x:expand_contractions(x))\n",
        "\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "def remove_stopwords(article):\n",
        "    \"Return the articel after remvoing stopwords\"\n",
        "    article_tokens = word_tokenize(article) \n",
        "    filtered_article = [word for word in article_tokens if not word in stop_words] \n",
        "    return \" \".join(filtered_article)\n",
        "\n",
        "\n",
        "#removing stopwords\n",
        "medium_articles.text = medium_articles.text.apply(remove_stopwords)\n",
        "\n",
        "#removing Punctuations \n",
        "medium_articles.text = medium_articles.text.apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x))\n",
        "\n",
        "#removing digits\n",
        "medium_articles.text = medium_articles.text.apply(lambda x: re.sub('\\w*\\d\\w*','', x))\n",
        "\n",
        "def remove_extra_marks(article):\n",
        "    extra_keys = [\"’\",\"—\",\"”\",\"“\"]\n",
        "    article_tokens = word_tokenize(article) \n",
        "    filtered_article = [word for word in article_tokens if not word in extra_keys] \n",
        "    return \" \".join(filtered_article)\n",
        "    \n",
        "medium_articles.text = medium_articles.text.apply(remove_extra_marks)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cfwy0zuz3zd"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "def lemmatize(text):\n",
        "    doc = nlp(text)\n",
        "    tokens = [token for token in doc]\n",
        "    return  \" \".join([token.lemma_ for token in doc])\n",
        "\n",
        "#lemmatize the articles\n",
        "medium_articles.text = medium_articles.text.apply(lemmatize)\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QJFQpdL0OQ-"
      },
      "source": [
        "#tokenize articles\n",
        "tokeize_article = medium_articles.text.apply(lambda x : x.split())\n",
        "id2word = corpora.Dictionary(tokeize_article)\n",
        "\n",
        "# Create Corpus\n",
        "texts = tokeize_article\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "\n",
        "#printing 50 words from the text corpus\n",
        "corpus_example = [[(id2word[id], freq) for id, freq in cp] for cp in corpus[:2]]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxfwz0TG0RSD"
      },
      "source": [
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=5, \n",
        "                                           random_state=100,\n",
        "                                           update_every=1,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha='symmetric',\n",
        "                                           per_word_topics=True,\n",
        "                                           eta = 0.6)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSrM7uZo0cf3",
        "outputId": "be36b696-e1eb-48ab-fec1-1c5091e18254"
      },
      "source": [
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=tokeize_article, dictionary=id2word, coherence='u_mass')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Coherence Score:  -1.69465075371047\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zd41-WeA20ME"
      },
      "source": [
        "def calculate_umass_score(n, alpha, beta):\n",
        "  lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=n, \n",
        "                                           random_state=100,\n",
        "                                           update_every=1,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha=alpha,\n",
        "                                           per_word_topics=True,\n",
        "                                           eta = beta)\n",
        "  coherence_model_lda = CoherenceModel(model=lda_model, texts=tokeize_article, dictionary=id2word, coherence='u_mass')\n",
        "  coherence_lda = coherence_model_lda.get_coherence()\n",
        "  return coherence_lda"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l98QzaeO4bxA",
        "outputId": "332817e2-91e8-4150-de12-d6f5a99801b4"
      },
      "source": [
        "no_of_topics = [2,5,7,10,12,14]\n",
        "alpha_list = ['symmetric',0.3,0.5,0.7]\n",
        "beta_list = ['auto',0.3,0.5,0.7]\n",
        "\n",
        "# parameter searching\n",
        "best_score = -100\n",
        "best_topic_n = None\n",
        "best_a =None\n",
        "best_b=None\n",
        "for n in no_of_topics:\n",
        "    for alpha in alpha_list:\n",
        "        for beta in beta_list:\n",
        "            coherence_score = calculate_umass_score(n, alpha, beta)\n",
        "\n",
        "            if coherence_score > best_score:\n",
        "              best_score = coherence_score\n",
        "              best_topic_n = n\n",
        "              best_a = alpha\n",
        "              best_b=beta\n",
        "\n",
        "print(\"best number of topics: \",n, \"best_alpha: \",best_a, \"best_b:\",best_b)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best number of topics:  14 best_alpha:  symmetric best_b: auto\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwmefSQw8hEQ",
        "outputId": "191481d3-c497-4e30-cab9-cdb5ceb68272"
      },
      "source": [
        "best_score"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.31741277162613524"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JmA7Tjst8kP9",
        "outputId": "f38a75cc-3297-473f-84e8-732e1ecf3dd1"
      },
      "source": [
        "best_a"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'symmetric'"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "iid8o0Hu8oyX",
        "outputId": "5a546b3b-75c0-4668-89d5-200150ba657b"
      },
      "source": [
        "best_b"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'auto'"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    }
  ]
}