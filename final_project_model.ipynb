{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_project_cvae_version.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3e61YWVJGXF"
      },
      "source": [
        "!pip install cvae"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mif71g64bL1A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5599afae-6882-4ba6-8384-f9bfbf984ce2"
      },
      "source": [
        "!pip install --user -U nltk"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.6.3-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.62.3)\n",
            "Installing collected packages: nltk\n",
            "\u001b[33m  WARNING: The script nltk is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "Successfully installed nltk-3.6.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKiv7ojAyJfR"
      },
      "source": [
        "!pip install sentence-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BYbHhnFJdfD",
        "outputId": "e627046c-bf38-4e38-8183-2022b13aa7cf"
      },
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils\n",
        "import torch.distributions\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt; plt.rcParams['figure.dpi'] = 200\n",
        "from torch.utils.data import Dataset, TensorDataset,DataLoader\n",
        "from sklearn.cluster import DBSCAN\n",
        "# import hdbscan\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from cvae import cvae\n",
        "from sklearn.cluster import KMeans\n",
        "from gensim.test.utils import common_corpus, common_dictionary\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from pprint import pprint\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4IJgwi7I8vh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49d50cae-7274-4d96-fd0b-b6b839b8167b"
      },
      "source": [
        "# from sklearn.datasets import fetch_20newsgroups\n",
        "news = fetch_20newsgroups(subset='all')['data']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_XW7UCrJBSP"
      },
      "source": [
        "tm = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y92xa8d9dqDp"
      },
      "source": [
        "class TopicModelling:\n",
        "  def __init__(self,data,sentence_transformer):\n",
        "    self.data = data \n",
        "    self.model = sentence_transformer\n",
        "    # self.algorithm = compression_algorithim\n",
        "    # self.latent_dim = compressed_dim\n",
        "  \n",
        "  def valid_input(self):\n",
        "    if not isinstance(self.data,list):\n",
        "      raise ValueError(\"The data passed to the model should be an array of strings\")\n",
        "    if len(self.data) < 1:\n",
        "      raise ValueError(\"The data list is empty\")\n",
        "    if not isinstance(self.data[0],str):\n",
        "      raise ValueError(\"The data passed to the model should be an array of strings\") \n",
        "  \n",
        "  def reduce_dimensions(self,size,embeddings):\n",
        "    embedder = cvae.CompressionVAE(embeddings,dim_latent=size)\n",
        "    embedder.train()\n",
        "    z = embedder.embed(embeddings)\n",
        "    \n",
        "    return z\n",
        "\n",
        "  def clusterKmeans(self,n_clusters,data):\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(data)\n",
        "    return kmeans\n",
        "\n",
        "  def class_based_tf_idf(self,documents, m, ngram_range=(1, 1)):\n",
        "    count = CountVectorizer(ngram_range=ngram_range, stop_words=\"english\").fit(documents)\n",
        "    t = count.transform(documents).toarray()\n",
        "    w = t.sum(axis=1)\n",
        "    tf = np.divide(t.T, w)\n",
        "    sum_t = t.sum(axis=0)\n",
        "    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n",
        "    tf_idf = np.multiply(tf, idf)\n",
        "\n",
        "    return tf_idf, count\n",
        "  \n",
        "  def extract_top_n_words_per_topic(self,tf_idf, count, docs_per_topic, n=20):\n",
        "    words = count.get_feature_names()\n",
        "    labels = list(docs_per_topic.Topic)\n",
        "    tf_idf_transposed = tf_idf.T\n",
        "    indices = tf_idf_transposed.argsort()[:, -n:]\n",
        "    top_n_words = {label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1] for i, label in enumerate(labels)}\n",
        "    return top_n_words\n",
        "\n",
        "  def extract_topic_sizes(self,df):\n",
        "    topic_sizes = (df.groupby(['Topic'])\n",
        "                     .Doc\n",
        "                     .count()\n",
        "                     .reset_index()\n",
        "                     .rename({\"Topic\": \"Topic\", \"Doc\": \"Size\"}, axis='columns')\n",
        "                     .sort_values(\"Size\", ascending=False))\n",
        "    return topic_sizes\n",
        "  \n",
        "  def visualize_2d_kmeans(self,embedings,n_clusters):\n",
        "    z = self.reduce_dimensions(2,embedings)\n",
        "    cluster = self.clusterKmeans(n_clusters,z)\n",
        "    kmeans_result = pd.DataFrame(z, columns=['x', 'y'])\n",
        "    kmeans_result['labels'] = cluster.labels_\n",
        "    fig, ax = plt.subplots(figsize=(20, 10))\n",
        "    plt.scatter(kmeans_result.x, kmeans_result.y, c=kmeans_result.labels, s=0.05, cmap='hsv_r')\n",
        "    plt.colorbar()\n",
        "  \n",
        "  def train(self,reduced_dim_size,n_clusters):\n",
        "    self.n_clusters = n_clusters\n",
        "    self.valid_input()\n",
        "    self.embedings = self.model.encode(self.data, show_progress_bar=True)\n",
        "    z = self.reduce_dimensions(reduced_dim_size,self.embedings)\n",
        "    cluster = self.clusterKmeans(n_clusters,z)\n",
        "    self.labels = cluster.labels_\n",
        "    return cluster.labels_\n",
        "\n",
        "\n",
        "  def get_topics(self):\n",
        "    docs_df = pd.DataFrame(self.data, columns=[\"Doc\"])\n",
        "    docs_df['Topic'] = self.labels\n",
        "    docs_df['Doc_ID'] = range(len(docs_df))\n",
        "    docs_per_topic = docs_df.groupby(['Topic'], as_index = False).agg({'Doc': ' '.join})\n",
        "    tf_idf, count = self.class_based_tf_idf(docs_per_topic.Doc.values, m=len(self.data))\n",
        "    top_n_words = self.extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20)\n",
        "    topic_sizes = self.extract_topic_sizes(docs_df); topic_sizes.head(10)\n",
        "    return top_n_words,topic_sizes\n",
        "  \n",
        "  def get_keyords_by_topic(self,top_words,word_dict):\n",
        "    topic_keywords =[]\n",
        "    for i in range(self.n_clusters):\n",
        "      keywords = [x[0] for x in top_words[i][:10] if x[0] in word_dict.token2id  ]\n",
        "      topic_keywords.append(keywords)\n",
        "    return topic_keywords\n",
        "  \n",
        "\n",
        "  \n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOXgWh7ZeTh6"
      },
      "source": [
        "##Medium Articles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwB1XEz4RDFT"
      },
      "source": [
        "medium_articles = pd.read_csv(\"articles.csv\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLlojlvdRv5Z",
        "outputId": "3a7aac9f-5783-4337-a06f-15ca16723437"
      },
      "source": [
        "medium_articles.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(337, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SJBO1InbTVX"
      },
      "source": [
        "def prepare_medium_articles():\n",
        "  contractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n",
        "                     \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n",
        "                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n",
        "                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n",
        "                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
        "                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n",
        "                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n",
        "                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n",
        "                     \"I'd\": \"I would\", \"I'd've\": \"I would have\",\"I'll\": \"I will\",\n",
        "                     \"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\", \"isn't\": \"is not\",\n",
        "                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n",
        "                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n",
        "                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n",
        "                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n",
        "                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
        "                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n",
        "                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
        "                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n",
        "                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n",
        "                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n",
        "                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n",
        "                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n",
        "                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n",
        "                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n",
        "                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n",
        "                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n",
        "                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n",
        "                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n",
        "                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n",
        "                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n",
        "                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n",
        "                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n",
        "                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
        "                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
        "                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n",
        "                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n",
        "                     \"you've\": \"you have\"}\n",
        "\n",
        "  medium_articles = pd.read_csv(\"articles.csv\")\n",
        "  # medium_articles.text = medium_articles.text.apply(lambda t : t.lower())\n",
        "  contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
        "  def expand_contractions(text,contractions_dict=contractions_dict):\n",
        "    def replace(match):\n",
        "        return contractions_dict[match.group(0)]\n",
        "    return contractions_re.sub(replace, text)\n",
        "  medium_articles.text = medium_articles.text.apply(lambda x:expand_contractions(x))\n",
        "  #removing digits\n",
        "  medium_articles.text = medium_articles.text.apply(lambda x: re.sub('\\w*\\d\\w*','', x))\n",
        "  \n",
        "  tokeize_article = medium_articles.text.apply(lambda x : x.split())\n",
        "  id2word = corpora.Dictionary(tokeize_article)\n",
        "  # Create Corpus\n",
        "  texts = tokeize_article\n",
        "  # Term Document Frequency\n",
        "  corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "  data = []\n",
        "  for i in range(medium_articles.shape[0]):\n",
        "    sents = nltk.sent_tokenize(medium_articles['text'][i])\n",
        "    for sent in sents:\n",
        "      data.append(sent)\n",
        "\n",
        "  return data, id2word, corpus\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctSBWL2YcVEY"
      },
      "source": [
        "d,worddict,corpus = prepare_medium_articles()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWIpr4zIe21F",
        "outputId": "2637034c-c325-4690-9d63-28da8b78025d"
      },
      "source": [
        "len(d)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32605"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7NqhbI7eIhc"
      },
      "source": [
        "model = TopicModelling(d,tm)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rdgLoXLea6D"
      },
      "source": [
        "# first parameter is the reduced space of the data\n",
        "# second parameter is the number of topics \n",
        "model.train(5,5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mZnW8YwetgZ"
      },
      "source": [
        "top_words,topic_sizes = model.get_topics()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CJ4NRc-1tJP"
      },
      "source": [
        "Using the extracted topics to measure topic coherence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrXb7rKtbf-v"
      },
      "source": [
        "cm = CoherenceModel(topics=model.get_keyords_by_topic(top_words,worddict), corpus=corpus, dictionary=worddict, coherence='u_mass')\n",
        "coherence = cm.get_coherence() "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FU1TDY6ue_nC",
        "outputId": "fe5a1712-9850-4648-f46a-4ebc3f109a94"
      },
      "source": [
        "# best loss so far LDA does not beat this score \n",
        "# out of the box umass score is better for this model than an unoptimised LDA model \n",
        "# The LDA model has a good umass score -1.69465075371047 \n",
        "coherence"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.427210028288895"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}